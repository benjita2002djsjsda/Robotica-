/// M√≥dulo principal del proyecto de an√°lisis MDP
///
/// Implementa un an√°lisis completo de un Proceso de Decisi√≥n de Markov incluyendo:
/// - C√°lculo de pol√≠ticas √≥ptimas con Q-Value Iteration (Q(s,a))
/// - Simulaci√≥n visual interactiva del agente
/// - An√°lisis de robustez bajo diferentes niveles de ruido
/// - Generaci√≥n de visualizaciones y reportes en CSV
/// - Evaluaci√≥n de rendimiento bajo m√∫ltiples par√°metros
///
/// ALGORITMO IMPLEMENTADO: Q-Value Iteration
/// - Calcula Q(s,a) para todos los pares estado-acci√≥n
/// - Pol√≠tica √≥ptima: œÄ(s) = argmax_a Q(s,a)
/// - Valores de estado: V(s) = max_a Q(s,a)
mod config;
mod experimentos;
mod mdp_model;
mod plot_utils;
mod robustness;
mod simulation;
mod transition_matrices;

use config::obtener_recompensas;
use mdp_model::q_value_iteration;
use plot_utils::{graficar_recompensas_barras, graficar_resultados_finales, leer_recompensas_csv};
use robustness::{construir_modelo_ruido, MODELOS_ROBUSTEZ};
use simulation::{ejecutar_simulacion, simulacion_1000_pasos};
use std::collections::HashMap;
use transition_matrices::guardar_matrices_transicion_csv;

/// Funci√≥n principal - Orquesta el an√°lisis completo del MDP usando Q-Value Iteration

#[macroquad::main("Simulacion MDP Robot - Q-Value Iteration")]
async fn main() {
    // Configuraci√≥n del espacio de par√°metros para el an√°lisis
    let factores_landa = vec![0.86, 0.90, 0.94, 0.98]; // Factores de descuento a evaluar
    let probabilidades_exito = vec![0.5, 0.7, 0.8, 0.9]; // Niveles de ruido en movimientos

    let mut resumen_1000_pasos = vec![];
    let mut politicas_optimas = vec![]; // Almac√©n de pol√≠ticas para an√°lisis de robustez
    let mut recompensas_map = obtener_recompensas();

    // === FASE 1: C√ÅLCULO DE POL√çTICAS √ìPTIMAS Y EVALUACI√ìN INICIAL ===
    for &landa in &factores_landa {
        println!(
            "\n=== Ejecutando Q-Value Iteration para Œª = {:.2} ===",
            landa
        );
        println!("Resultados de simulaciones de 1000 pasos:");
        println!("----------------------------------------");

        // C√°lculo de la pol√≠tica √≥ptima usando Q-Value Iteration Q(s,a)
        let (_q_valores, politica, v_valores) = q_value_iteration(landa, Some(0.001), None);

        // Convertir v_valores de HashMap<String, f64> a HashMap<&str, f64> para compatibilidad
        let valores: HashMap<&str, f64> = v_valores.iter().map(|(k, &v)| (k.as_str(), v)).collect();

        // Evaluaci√≥n de rendimiento bajo diferentes niveles de ruido
        for &prob in &probabilidades_exito {
            let (metas, peligros, recompensa) = simulacion_1000_pasos(&politica, 1000, prob);
            resumen_1000_pasos.push((landa, prob, recompensa));

            println!(
                "Œª={:.2} - Prob √©xito {:.0}% Recompensa: {:.2} (Meta: {}, Peligros: {})",
                landa,
                prob * 100.0,
                recompensa,
                metas,
                peligros
            );
        }

        // Reporte de valores de estado calculados
        println!("\nValor de los estados:");
        let mut keys: Vec<_> = valores.keys().collect();
        keys.sort();
        for k in keys {
            println!("{}: {:.2}", k, valores[k]);
        }

        // Visualizaci√≥n de la pol√≠tica √≥ptima en formato de mapa
        println!("\nPol√≠tica √≥ptima para Œª={}:", landa);
        println!("\nMapa de pol√≠tica √≥ptima:");
        println!("-------------------------");
        for fila in config::MAPA_ESTADOS.iter() {
            for estado in fila {
                let simbolo = if config::OBSTACULOS.contains(estado) {
                    "‚¨õ" // Obst√°culos
                } else if *estado == config::ESTADO_META {
                    "üéØ" // Estado meta
                } else if config::ESTADOS_PELIGRO.contains(estado) {
                    "‚ö†Ô∏è" // Estados peligrosos
                } else {
                    // Direcci√≥n √≥ptima seg√∫n la pol√≠tica
                    match politica.get(*estado).map(String::as_str) {
                        Some("N") => "‚Üë",
                        Some("S") => "‚Üì",
                        Some("E") => "‚Üí",
                        Some("O") => "‚Üê",
                        _ => " ",
                    }
                };
                print!("{} ", simbolo);
            }
            println!();
        }
        println!("-------------------------");
        println!("Leyenda: ‚¨õ Obst√°culo | üéØ Meta | ‚ö†Ô∏è Peligro | ‚Üë‚Üì‚Üê‚Üí Direcci√≥n √≥ptima");

        // === FASE 2: SIMULACI√ìN VISUAL INTERACTIVA ===
        println!("\n‚Üí Iniciando simulaci√≥n visual (siguiendo pol√≠tica √≥ptima)...");
        ejecutar_simulacion(&politica, 100, &mut recompensas_map, landa).await;

        // Almacenar pol√≠tica para an√°lisis de robustez posterior
        politicas_optimas.push((landa, politica.clone()));
    }

    // === FASE 3: EVALUACI√ìN DE ROBUSTEZ BAJO DIFERENTES MODELOS DE RUIDO ===
    println!("\n=== EVALUANDO ROBUSTEZ CON MODELOS ESPEC√çFICOS ===");
    println!("Evaluando con: (10%,80%,10%), (5%,90%,5%), (15%,70%,15%), (25%,50%,25%)");

    // An√°lisis de robustez: c√≥mo se adapta cada pol√≠tica a diferentes niveles de ruido
    for (lambda, _politica_base) in &politicas_optimas {
        println!("\n--- Evaluaci√≥n robustez Œª={:.2} ---", lambda);

        for (izq, centro, der) in MODELOS_ROBUSTEZ.iter() {
            // Rec√°lculo de pol√≠tica √≥ptima bajo el modelo de ruido espec√≠fico
            let modelo_ruido = construir_modelo_ruido(*izq, *centro, *der);
            let (_q_valores, politica_adaptada, _v_valores) =
                q_value_iteration(*lambda, Some(0.001), Some(&modelo_ruido));

            // Evaluaci√≥n de rendimiento con la pol√≠tica adaptada al ruido
            let (metas, peligros, _recompensa) =
                simulacion_1000_pasos(&politica_adaptada, 1000, *centro);

            println!(
                "Modelo ({:.0}%,{:.0}%,{:.0}%): {} metas, {} peligros",
                izq * 100.0,
                centro * 100.0,
                der * 100.0,
                metas,
                peligros
            );
        }
    }

    // === FASE 4: GENERACI√ìN DE REPORTES Y VISUALIZACIONES ===
    println!("\n=== GENERANDO GR√ÅFICOS Y GUARDANDO DATOS ===");

    // Exportaci√≥n de datos de simulaciones de 1000 pasos
    experimentos::guardar_recompensas_csv(&resumen_1000_pasos, "datos_1000_pasos.csv")
        .expect("Error guardando datos de 1000 pasos");

    // Generaci√≥n de gr√°ficos de barras comparativos
    if let Err(e) = graficar_recompensas_barras(&resumen_1000_pasos) {
        eprintln!("Error al generar gr√°fico de barras: {:?}", e);
    }

    // Simulaci√≥n exhaustiva y guardado de resultados completos
    experimentos::simular_y_guardar_csv(
        &factores_landa,
        &probabilidades_exito,
        1000, // Episodios por combinaci√≥n de par√°metros
        100,  // Pasos m√°ximos por episodio
        "resultados_simulacion.csv",
    );

    // Generaci√≥n de visualizaciones finales basadas en datos CSV
    let resumen_recompensas_csv = leer_recompensas_csv("resultados_simulacion.csv");
    if let Err(e) = graficar_resultados_finales(&resumen_recompensas_csv) {
        eprintln!("Error al graficar resultados finales: {:?}", e);
    }

    // Exportaci√≥n de matrices de transici√≥n para an√°lisis externo
    guardar_matrices_transicion_csv();
}
